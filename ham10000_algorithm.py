# -*- coding: utf-8 -*-
"""HAM10000 Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dS9kkZo7_9YuSj-5iU1WyxAjoltFhD9B

# Imports
---
"""

import os
import pandas as pd
import shutil
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# For warnings control
import warnings
warnings.filterwarnings('ignore')

"""## Define Constants"""

BASE_DIR = "skin_cancer_dataset"  # Base directory for organized dataset
METADATA_PATH = "HAM10000_metadata.csv"  # Path to metadata CSV
IMAGES_PATH = "HAM10000_images"  # Directory containing the original images

"""## Load Data"""

# Read the metadata CSV file
df = pd.read_csv(METADATA_PATH)
print("Dataset Overview:")
print("-" * 50)
print(df.info())
print("\nFirst few rows of the dataset:")
display(df.head())

"""## Class Distribution"""

"""
Visualize the distribution of different skin lesion types
"""
# Create a bar plot of class distribution
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='dx')
plt.title('Distribution of Skin Lesion Types')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print class distribution
print("\nClass Distribution:")
print(df['dx'].value_counts())

"""## Directory Structure"""

"""
Create the necessary directories for organizing the dataset into train/val/test splits
"""
# Create base directory if it doesn't exist
os.makedirs(BASE_DIR, exist_ok=True)

# Create splits and class directories
splits = ['train', 'val', 'test']
classes = df['dx'].unique()

for split in splits:
    for class_name in classes:
        os.makedirs(os.path.join(BASE_DIR, split, class_name), exist_ok=True)

print("Directory structure created successfully!")

"""## Split Dataset"""

"""
Split the dataset into training, validation, and test sets while maintaining class distribution
"""
# Create stratified train/temp split (70%, 30%)
df_train, df_temp = train_test_split(df, test_size=0.3, stratify=df['dx'], random_state=42)

# Split temp into validation and test sets (15%/15% total)
df_val, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['dx'], random_state=42)

# Print split sizes
print("Dataset Split Sizes:")
print(f"Training set: {len(df_train)} images ({len(df_train)/len(df)*100:.1f}%)")
print(f"Validation set: {len(df_val)} images ({len(df_val)/len(df)*100:.1f}%)")
print(f"Test set: {len(df_test)} images ({len(df_test)/len(df)*100:.1f}%)")

"""## Copy images to their directories"""

print("Current directory contents:")
print(os.listdir())

# Define paths
IMAGES_PATH = "HAM10000_images"  # Directory containing the original images
BASE_DIR = "skin_cancer_dataset"  # Base directory for organized dataset

def copy_images(df_split, split_name):
    """Copy images to appropriate directories based on their split and class"""
    for idx, row in df_split.iterrows():
        src_path = os.path.join(IMAGES_PATH, f"{row['image_id']}.jpg")
        dst_path = os.path.join(BASE_DIR, split_name, row['dx'], f"{row['image_id']}.jpg")

        if os.path.exists(src_path):
            shutil.copy2(src_path, dst_path)
        else:
            print(f"Warning: Image {row['image_id']} not found")

print("Copying images...")
copy_images(df_train, 'train')
copy_images(df_val, 'val')
copy_images(df_test, 'test')
print("Images copied successfully!")

"""## Verify Dataset Structure"""

"""
Analyze the prepared dataset structure and verify class distribution in each split
"""
def analyze_split(split_name):
    """
    Analyze the distribution of images in a specific split

    Parameters:
    split_name: String, name of the split to analyze
    """
    split_path = os.path.join(BASE_DIR, split_name)
    class_counts = {}

    # Count images in each class
    for class_name in os.listdir(split_path):
        class_path = os.path.join(split_path, class_name)
        if os.path.isdir(class_path):
            count = len(os.listdir(class_path))
            class_counts[class_name] = count

    return pd.Series(class_counts)

# Analyze each split
splits = ['train', 'val', 'test']
split_distributions = {}

for split in splits:
    split_distributions[split] = analyze_split(split)

# Create a DataFrame with all split distributions
distribution_df = pd.DataFrame(split_distributions)

# Display the distribution
print("Final Distribution of Images:")
display(distribution_df)

"""## Visualize Final Distribution"""

"""
Create a visualization of the class distribution across splits
"""
# Plot distribution across splits
plt.figure(figsize=(12, 6))
distribution_df.plot(kind='bar')
plt.title('Class Distribution Across Splits')
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.legend(title='Split')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Save Split Information"""

"""
Save the split information for future reference
"""
# Save split DataFrames to CSV
df_train.to_csv(os.path.join(BASE_DIR, 'train_metadata.csv'), index=False)
df_val.to_csv(os.path.join(BASE_DIR, 'val_metadata.csv'), index=False)
df_test.to_csv(os.path.join(BASE_DIR, 'test_metadata.csv'), index=False)

"""## Dataset Summary"""

"""
Print a comprehensive summary of the prepared dataset
"""
print("Dataset Preparation Summary")
print("-" * 50)
print(f"Total number of images: {len(df)}")
print(f"Number of classes: {len(classes)}")
print("\nClass names and their meanings:")
class_meanings = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}
for code, meaning in class_meanings.items():
    print(f"{code}: {meaning}")

print("\nDataset is now ready for model training!")

"""## Model Training"""

import tensorflow as tf
import PIL
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

PIL.ImageFile.LOAD_TRUNCATED_IMAGES = True

# Parameters
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20

# Data generators
train_datagen = ImageDataGenerator(
   rescale=1./255,
   rotation_range=20,
   width_shift_range=0.2,
   height_shift_range=0.2,
   shear_range=0.2,
   zoom_range=0.2,
   horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)

# Load data
train_generator = train_datagen.flow_from_directory(
   f'{BASE_DIR}/train',
   target_size=(IMG_SIZE, IMG_SIZE),
   batch_size=BATCH_SIZE,
   class_mode='categorical'
)

validation_generator = val_datagen.flow_from_directory(
   f'{BASE_DIR}/val',
   target_size=(IMG_SIZE, IMG_SIZE),
   batch_size=BATCH_SIZE,
   class_mode='categorical'
)

# Create model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
for layer in base_model.layers:
   layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Compile and train
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
   train_generator,
   validation_data=validation_generator,
   epochs=EPOCHS,
   callbacks=[
       tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),
       tf.keras.callbacks.EarlyStopping(patience=3)
   ]
)

# Save model
model.save('skin_cancer_model.keras')

# Visualize results
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.tight_layout()
plt.show()

"""## Visualize Training Results"""

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.tight_layout()
plt.show()